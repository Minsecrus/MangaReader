#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Manga OCR Service (Hybrid Mode)
è‡ªåŠ¨æ£€æµ‹ï¼šä¼˜å…ˆåŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œåˆ™åœ¨çº¿ä¸‹è½½/åŠ è½½
"""

import sys
import json
import base64
import os
import argparse
from io import BytesIO
from PIL import Image
from manga_ocr import MangaOcr
from sudachipy import dictionary, SplitMode

# å¢åŠ  stdout çš„ç¼“å†²è®¾ç½®ï¼Œé˜²æ­¢æ‰“å°è¿›åº¦æ¡æ—¶å¡ä½
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", line_buffering=True)
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", line_buffering=True)

# --- è¯æ€§æ˜ å°„è¡¨ (æ—¥è¯­ -> å‰ç«¯ç±»å‹) ---
# Sudachi çš„è¯æ€§éå¸¸è¯¦ç»†ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ç®€åŒ–ä¸ºå‰ç«¯ TokenizedWords.vue éœ€è¦çš„ç±»å‹
POS_MAPPING = {
    "åè©": "noun",
    "ä»£åè©": "noun",
    "å‹•è©": "verb",
    "å½¢å®¹è©": "adjective",
    "å½¢çŠ¶è©": "adjective",  # å½¢å®¹åŠ¨è¯
    "å‰¯è©": "adverb",  # å‰ç«¯æš‚æ—¶æ²¡å®šä¹‰ï¼Œå¯ä»¥å½’ä¸º other æˆ–åŠ ç±»å‹
    "åŠ©è©": "particle",
    "åŠ©å‹•è©": "particle",  # ä¹Ÿå¯ä»¥å½’ä¸º verbï¼Œè§†æƒ…å†µè€Œå®š
    "æ„Ÿå‹•è©": "other",
    "æ¥é ­è¾": "other",
    "æ¥å°¾è¾": "other",
    "è¨˜å·": "other",
}


def log_message(message):
    """è¾“å‡ºæ—¥å¿—åˆ° stderr"""
    print(f"[OCR Service] {message}", file=sys.stderr, flush=True)


def send_response(response):
    """å‘é€ JSON å“åº”åˆ° stdout"""
    print(json.dumps(response, ensure_ascii=False), flush=True)


def check_local_model_integrity(model_path):
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    if not os.path.exists(model_path):
        return False

    # æ£€æŸ¥å¿…è¦çš„é…ç½®æ–‡ä»¶å’Œè¯è¡¨
    required_files = [
        "config.json",
        "special_tokens_map.json",
        "tokenizer_config.json",
        "vocab.txt",  # æˆ–è€… spiece.modelï¼Œè§†æ¨¡å‹è€Œå®šï¼Œmanga-ocré€šå¸¸æ˜¯vocab.txt
    ]

    for f in required_files:
        if not os.path.exists(os.path.join(model_path, f)):
            log_message(f"Missing file: {f}")
            return False

    # æ£€æŸ¥æƒé‡æ–‡ä»¶ (safetensors æˆ– bin åªè¦æœ‰ä¸€ä¸ªå°±è¡Œ)
    has_safetensors = os.path.exists(os.path.join(model_path, "model.safetensors"))
    has_bin = os.path.exists(os.path.join(model_path, "pytorch_model.bin"))

    if not (has_safetensors or has_bin):
        log_message("Missing model weights (model.safetensors or pytorch_model.bin)")
        return False

    return True


def main():
    log_message("Starting OCR service...")

    parser = argparse.ArgumentParser()
    parser.add_argument("--model-dir", type=str, help="Path to the OCR model directory")
    args, unknown = parser.parse_known_args()

    mocr = None
    sudachi_tokenizer = None
    sudachi_init_error = None

    # 2. åˆå§‹åŒ– Sudachi åˆ†è¯å™¨
    log_message("Initializing Sudachi Tokenizer...")
    try:
        # åŠ è½½æ ¸å¿ƒè¯å…¸
        sudachi_tokenizer = dictionary.Dictionary(dict="core").create()
        mode = SplitMode.C  # Mode C æ˜¯æœ€é•¿åˆ†å‰²ï¼Œé€‚åˆé˜…è¯» (Mode A æ˜¯æœ€ç»†åˆ†å‰²)
        log_message("âœ… Sudachi Initialized.")
    except Exception as e:
        error_str = str(e)
        sudachi_init_error = error_str
        log_message(f"âŒ Sudachi Init Failed: {error_str}")
        sudachi_tokenizer = None
        sudachi_tokenizer = None

    local_model_path = args.model_dir  # è·å–ä¼ å…¥çš„è·¯å¾„

    # 1. ä¼˜å…ˆå°è¯•åŠ è½½ä¼ å…¥çš„æœ¬åœ°è·¯å¾„
    if local_model_path:
        log_message(f"Checking model at: {local_model_path}")

        if check_local_model_integrity(local_model_path):
            log_message("âœ… Valid local model found. Loading offline mode...")
            try:
                mocr = MangaOcr(pretrained_model_name_or_path=local_model_path)
            except Exception as e:
                log_message(f"âš ï¸ Load failed: {e}")
        else:
            log_message(
                "â— Local model not found or incomplete. (Will use online mode)"
            )
    else:
        log_message("âš ï¸ No model path provided.")

    # 2. å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥ï¼Œèµ°åœ¨çº¿æ¨¡å¼ (é»˜è®¤ä¸‹è½½åˆ° Cç›˜ .cache)
    if mocr is None:
        log_message("ğŸŒ Connecting to HuggingFace (Online Mode)...")
        # å¯ä»¥åœ¨è¿™é‡ŒæŒ‡å®š cache_dir ä¹Ÿå¯ä»¥é»˜è®¤
        mocr = MangaOcr()

    log_message("âœ… Model loaded successfully!")
    send_response({"status": "ready"})

    # --- ä¸‹é¢ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜ ---
    log_message("Waiting for requests...")

    for line in sys.stdin:
        req_id = None
        try:
            line = line.strip()
            if not line:
                continue

            request = json.loads(line)
            req_id = request.get("id")  # è·å– ID
            command = request.get("command")

            # === 1. OCR è¯†åˆ« ===
            if command == "recognize":
                request_id = request.get("id")
                log_message(f"Processing request {request_id}")

                image_base64 = request.get("image", "")
                if "," in image_base64:
                    image_base64 = image_base64.split(",", 1)[1]

                image_data = base64.b64decode(image_base64)
                img = Image.open(BytesIO(image_data))

                text = mocr(img)
                log_message(f"Result: {text}")
                send_response({"id": request_id, "success": True, "text": text})

            # åˆ†è¯
            elif command == "tokenize":
                # log_message(f"Tokenizing {req_id}")
                log_message(f"DEBUG: Received tokenize request ID: {req_id}")
                text = request.get("text", "")

                if not sudachi_tokenizer:
                    # å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼ŒæŠŠå…·ä½“çš„ sudachi_init_error è¿”å›ç»™å‰ç«¯
                    error_msg = f"Tokenizer init failed: {sudachi_init_error or 'Unknown error'}"
                    log_message(error_msg)  # åœ¨åå°ä¹Ÿæ‰“å°ä¸€ä¸‹

                    send_response({"id": req_id, "success": False, "error": error_msg})
                    continue  # è·³è¿‡æœ¬æ¬¡å¾ªç¯

                try:
                    # ğŸ›‘ è°ƒè¯•æ—¥å¿— 2ï¼šå¼€å§‹è®¡ç®—
                    log_message(f"DEBUG: Start tokenizing text length: {len(text)}")

                    tokens = []
                    results = sudachi_tokenizer.tokenize(text, mode)

                    # ğŸ›‘ è°ƒè¯•æ—¥å¿— 3ï¼šè®¡ç®—å®Œæˆï¼Œå¼€å§‹æ ¼å¼åŒ–
                    log_message(f"DEBUG: Tokenized finished, count: {len(results)}")

                    for t in results:
                        pos_list = t.part_of_speech()
                        main_pos = pos_list[0]
                        frontend_type = POS_MAPPING.get(main_pos, "other")
                        tokens.append(
                            {
                                "word": t.surface(),
                                "type": frontend_type,
                            }
                        )

                    # ğŸ›‘ è°ƒè¯•æ—¥å¿— 4ï¼šå‡†å¤‡å‘é€å“åº”
                    log_message("DEBUG: Sending response...")
                    send_response({"id": req_id, "success": True, "tokens": tokens})

                except Exception as e:
                    # ğŸ›‘ æ•è·åˆ†è¯è¿‡ç¨‹ä¸­çš„ç‰¹æ®Šé”™è¯¯
                    log_message(f"ERROR during tokenization: {str(e)}")
                    send_response({"id": req_id, "success": False, "error": str(e)})

            # å…¶ä»–å‘½ä»¤
            elif command == "ping":
                send_response({"success": True, "message": "pong"})

            elif command == "exit":
                sys.exit(0)

        except Exception as e:
            # æ•è·æ‰€æœ‰å¤„ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œé˜²æ­¢è¿›ç¨‹é€€å‡º
            error_msg = str(e)
            log_message(f"Error: {error_msg}")
            # å¦‚æœèƒ½è§£æå‡ºIDï¼Œå°½é‡å‘å›æŠ¥é”™ä¿¡æ¯
            try:
                req_id = json.loads(line).get("id")
                if req_id is not None:
                    send_response({"id": req_id, "success": False, "error": error_msg})
            except:
                pass


if __name__ == "__main__":
    main()
