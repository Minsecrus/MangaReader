# services/modules/translator/sakura_engine.py
import os
import threading
from .base import BaseTranslator
from huggingface_hub import hf_hub_download
from ..utils import log_message
import shutil

try:
    from llama_cpp import Llama
except ImportError:
    Llama = None


class SakuraEngine(BaseTranslator):
    def __init__(self, model_root_dir):
        path = os.path.join(model_root_dir, "sakura")
        super().__init__(path)

        # âœ… ä»“åº“ ID: ç¤¾åŒºé‡åŒ–ç‰ˆä»“åº“
        self.repo_id = "shing3232/Sakura-1.5B-Qwen2.5-v1.0-GGUF-IMX"

        # âœ… æ–‡ä»¶å: ä¿®æ­£ä¸ºçœŸå®å­˜åœ¨çš„ Q5KS ç‰ˆæœ¬ (1.26 GB)
        self.filename = "sakura-1.5b-qwen2.5-v1.0-Q5KS.gguf"
        self.model_file_path = os.path.join(self.model_dir, self.filename)

        self.llm = None
        self.lock = threading.Lock()

    def check_model_exists(self):
        path = self.model_file_path
        exists = os.path.exists(path)

        log_message(f"ğŸ” [Check] Path: {path}")
        log_message(f"ğŸ” [Check] Exists: {exists}")

        return exists

    def delete_model(self):
        # 1. å°è¯•é‡Šæ”¾å†…å­˜
        if self.llm:
            log_message("ğŸ”„ Unloading model from memory...")
            try:
                del self.llm
                self.llm = None
                self.is_ready = False
            except:
                pass

        # 2. åˆ é™¤ .gguf ä¸»æ¨¡å‹æ–‡ä»¶
        deleted_main = False
        if os.path.exists(self.model_file_path):
            try:
                os.remove(self.model_file_path)
                log_message(f"ğŸ—‘ï¸ Deleted main file: {self.filename}")
                deleted_main = True
            except Exception as e:
                log_message(f"âŒ Failed to delete main file: {e}")

        # 3. å½»åº•æ¸…ç† .cache æ–‡ä»¶å¤¹ (å…ƒæ•°æ®æ®‹ç•™)
        # self.model_dir å°±æ˜¯ .../models/translation/sakura
        cache_dir = os.path.join(self.model_dir, ".cache")
        if os.path.exists(cache_dir):
            try:
                shutil.rmtree(cache_dir)  # é€’å½’åˆ é™¤æ–‡ä»¶å¤¹
                log_message("ğŸ§¹ Cleaned up HuggingFace cache directory.")
            except Exception as e:
                log_message(f"âš ï¸ Failed to clean cache: {e}")

        return deleted_main

    def download_model(self, progress_callback=None):
        log_message(f"â¬‡ï¸ Downloading SakuraLLM to: {self.model_dir}")
        log_message(f"   File: {self.filename}")
        try:
            hf_hub_download(
                repo_id=self.repo_id,
                filename=self.filename,
                local_dir=self.model_dir,
                token=False,
            )
            log_message("âœ… SakuraLLM download complete.")
            return True
        except Exception as e:
            log_message(f"âŒ Download failed: {e}")
            raise e

    def initialize(self):
        if Llama is None:
            log_message("âŒ Error: llama-cpp-python not installed.")
            self.is_ready = False
            return

        model_path = self.model_file_path

        if not os.path.exists(model_path):
            log_message(f"âš ï¸ Initialize failed. Model not found at: {model_path}")
            self.is_ready = False
            return

        try:
            log_message(f"ğŸš€ Loading SakuraLLM (CPU Mode) from: {model_path}")

            self.llm = Llama(
                model_path=model_path, n_ctx=1024, n_threads=4, verbose=False
            )

            self.is_ready = True
            log_message("âœ… SakuraLLM Engine loaded.")
        except Exception as e:
            log_message(f"âŒ Failed to load Sakura: {e}")
            self.is_ready = False

    def translate(self, text):
        if not self.is_ready or not self.llm:
            raise Exception("Sakura Engine not ready")

        with self.lock:
            # Prompt æ ¼å¼ä¿æŒä¸å˜
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªè½»å°è¯´ç¿»è¯‘æ¨¡å‹ï¼Œå¯ä»¥æµç•…é€šé¡ºåœ°ä»¥æ—¥æœ¬è½»å°è¯´çš„é£æ ¼å°†æ—¥æ–‡ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ï¼Œå¹¶è”ç³»ä¸Šä¸‹æ–‡æ­£ç¡®ä½¿ç”¨äººç§°ä»£è¯ï¼Œä¸æ“…è‡ªæ·»åŠ åŸæ–‡ä¸­æ²¡æœ‰çš„ä»£è¯ã€‚"

            prompt = (
                f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
                f"<|im_start|>user\nå°†ä¸‹é¢çš„æ—¥æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼š{text}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )

            output = self.llm(
                prompt,
                max_tokens=512,
                stop=["<|im_end|>", "\n\n"],
                echo=False,
                temperature=0.1,
            )

            try:
                translation = output["choices"][0]["text"].strip()
                return translation
            except Exception as e:
                log_message(f"Sakura output error: {e}")
                return text
