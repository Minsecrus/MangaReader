# services/modules/translator/sakura_engine.py
import os
import threading
import shutil
import sys
import json
import contextlib
from .base import BaseTranslator
from huggingface_hub import hf_hub_download
from ..utils import log_message

import tqdm

try:
    from llama_cpp import Llama
except ImportError:
    Llama = None


# ç›´æ¥ Monkey Patch tqdm.tqdm ç±»çš„æ–¹æ³•
# è¿™æ ·æ— è®º huggingface_hub å¦‚ä½•å¼•ç”¨ tqdmï¼Œéƒ½ä¼šä½¿ç”¨æˆ‘ä»¬ä¿®æ”¹åçš„æ–¹æ³•
_original_init = tqdm.tqdm.__init__
_original_update = tqdm.tqdm.update


def _patched_init(self, *args, **kwargs):
    # 1. å¼ºåˆ¶å¼€å¯è¿›åº¦æ¡
    kwargs["disable"] = False
    # 2. å°†åŸå§‹çš„è§†è§‰è¿›åº¦æ¡é‡å®šå‘åˆ°ç©ºè®¾å¤‡ (devnull)
    # è¿™æ ·æ§åˆ¶å°å°±ä¸ä¼šå‡ºç° â–ˆâ–ˆâ–ˆâ–ˆâ–Œ è¿™ç§å­—ç¬¦ï¼Œé¿å…å¹²æ‰° JSON è§£æ
    kwargs["file"] = open(os.devnull, "w")

    _original_init(self, *args, **kwargs)

    # åˆå§‹åŒ–è‡ªå®šä¹‰çŠ¶æ€
    self.last_percent = -1


def _patched_update(self, n=1):
    # è°ƒç”¨åŸå§‹ update æ›´æ–°å†…éƒ¨è®¡æ•°å™¨
    _original_update(self, n)

    # è®¡ç®—ç™¾åˆ†æ¯”å¹¶è¾“å‡º JSON
    if self.total and self.total > 0:
        percent = (self.n / self.total) * 100

        # è¿‡æ»¤é¢‘ç‡ï¼šæ¯ 0.5% å‘é€ä¸€æ¬¡
        if int(percent * 2) > getattr(self, "last_percent", -1):
            self.last_percent = int(percent * 2)

            msg = {
                "type": "download_progress",
                "percent": round(percent, 1),
                "filename": self.desc or "model",
            }
            # 3. æ˜¾å¼å†™å…¥ stdout å¹¶ flushï¼Œç¡®ä¿ Electron èƒ½ç«‹å³æ”¶åˆ°
            sys.stdout.write(json.dumps(msg) + "\n")
            sys.stdout.flush()


@contextlib.contextmanager
def patch_tqdm():
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šåœ¨ä»£ç å—æ‰§è¡ŒæœŸé—´ï¼Œæ›¿æ¢ tqdm çš„è¡Œä¸º
    """
    # æ›¿æ¢æ–¹æ³•
    tqdm.tqdm.__init__ = _patched_init
    tqdm.tqdm.update = _patched_update

    try:
        yield
    finally:
        # è¿˜åŸæ–¹æ³•ï¼Œé¿å…å½±å“å…¶ä»–æ¨¡å—
        tqdm.tqdm.__init__ = _original_init
        tqdm.tqdm.update = _original_update


class SakuraEngine(BaseTranslator):
    def __init__(self, model_root_dir):
        path = os.path.join(model_root_dir, "sakura")
        super().__init__(path)

        self.repo_id = "shing3232/Sakura-1.5B-Qwen2.5-v1.0-GGUF-IMX"
        self.filename = "sakura-1.5b-qwen2.5-v1.0-Q5KS.gguf"

        # ä¸ºäº†æ›´å‡†ç¡®çš„åˆ¤æ–­ï¼Œæˆ‘ä»¬è¿™é‡Œè¿˜æ˜¯ç”¨ .gguf ç»“å°¾çš„æ–‡ä»¶è·¯å¾„
        self.model_file_path = os.path.join(self.model_dir, self.filename)

        self.llm = None
        self.lock = threading.Lock()

    def check_model_exists(self):
        # æ£€æŸ¥ç‰©ç†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        # æ³¨æ„ï¼šä½¿ç”¨ hf_hub_download åï¼Œå®é™…æ–‡ä»¶å¯èƒ½æ˜¯ä¸€ä¸ª symlink æŒ‡å‘ .cache
        # ä½† os.path.exists ä¼šè‡ªåŠ¨è¿½è¸ª symlinkï¼Œæ‰€ä»¥é€»è¾‘æ˜¯é€šç”¨çš„
        path = self.model_file_path
        exists = os.path.exists(path)
        log_message(f"ğŸ” [Check] Path: {path}")
        log_message(f"ğŸ” [Check] Exists: {exists}")
        return exists

    def delete_model(self):
        # 1. é‡Šæ”¾å†…å­˜
        if self.llm:
            log_message("ğŸ”„ Unloading model...")
            try:
                del self.llm
                self.llm = None
                self.is_ready = False
            except:
                pass

        deleted = False

        # 2. åˆ é™¤ä¸»æ–‡ä»¶ (å¦‚æœæ˜¯ symlink ä¼šåˆ æ‰ symlinkï¼Œå¦‚æœæ˜¯å®ä½“æ–‡ä»¶åˆ å®ä½“)
        if os.path.exists(self.model_file_path):
            try:
                os.remove(self.model_file_path)
                log_message(f"ğŸ—‘ï¸ Deleted model link/file: {self.filename}")
                deleted = True
            except Exception as e:
                log_message(f"âŒ Failed to delete model file: {e}")

        # 3. âœ… å…³é”®ï¼šæ¸…ç† .cache ç¼“å­˜
        # HuggingFace çš„é»˜è®¤ç¼“å­˜ç»“æ„é€šå¸¸åœ¨ models/translation/sakura/.cache
        # æˆ‘ä»¬æŠŠå®ƒæ•´ä¸ªå¹²æ‰ï¼Œè¿™æ ·æ‰æ˜¯çœŸçš„â€œå¸è½½â€
        cache_dir = os.path.join(self.model_dir, ".cache")
        if os.path.exists(cache_dir):
            try:
                shutil.rmtree(cache_dir)
                log_message("ğŸ§¹ Cleaned up HuggingFace cache directory.")
                deleted = True
            except Exception as e:
                log_message(f"âš ï¸ Failed to clean cache: {e}")

        return deleted

    def download_model(self, progress_callback=None):
        log_message(f"â¬‡ï¸ Downloading SakuraLLM via HuggingFace Hub...")
        log_message(f"   Repo: {self.repo_id}")

        try:
            # âœ… ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œåªåœ¨ä¸‹è½½æœŸé—´å¼€å¯â€œé—´è°æ¨¡å¼â€
            with patch_tqdm():
                file_path = hf_hub_download(
                    repo_id=self.repo_id,
                    filename=self.filename,
                    local_dir=self.model_dir,
                    # ä¸å†éœ€è¦ local_dir_use_symlinks=Falseï¼Œ
                    # ç°åœ¨çš„ç‰ˆæœ¬é»˜è®¤è¡Œä¸ºå¾ˆæ™ºèƒ½ï¼Œä¿ç•™ç¼“å­˜æœºåˆ¶æ›´å¥½
                    token=False,
                )

            log_message("âœ… SakuraLLM download complete.")
            return True
        except Exception as e:
            log_message(f"âŒ Download failed: {e}")
            raise e

    def initialize(self):
        if Llama is None:
            log_message("âŒ Error: llama-cpp-python not installed.")
            self.is_ready = False
            return

        model_path = self.model_file_path

        if not os.path.exists(model_path):
            log_message(f"âš ï¸ Initialize failed. Model not found at: {model_path}")
            self.is_ready = False
            return

        try:
            log_message(f"ğŸš€ Loading SakuraLLM (CPU Mode) from: {model_path}")

            self.llm = Llama(
                model_path=model_path, n_ctx=1024, n_threads=4, verbose=False
            )

            self.is_ready = True
            log_message("âœ… SakuraLLM Engine loaded.")
        except Exception as e:
            log_message(f"âŒ Failed to load Sakura: {e}")
            self.is_ready = False

    def translate(self, text):
        if not self.is_ready or not self.llm:
            raise Exception("Sakura Engine not ready")

        with self.lock:
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªè½»å°è¯´ç¿»è¯‘æ¨¡å‹ï¼Œå¯ä»¥æµç•…é€šé¡ºåœ°ä»¥æ—¥æœ¬è½»å°è¯´çš„é£æ ¼å°†æ—¥æ–‡ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ï¼Œå¹¶è”ç³»ä¸Šä¸‹æ–‡æ­£ç¡®ä½¿ç”¨äººç§°ä»£è¯ï¼Œä¸æ“…è‡ªæ·»åŠ åŸæ–‡ä¸­æ²¡æœ‰çš„ä»£è¯ã€‚"

            prompt = (
                f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
                f"<|im_start|>user\nå°†ä¸‹é¢çš„æ—¥æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼š{text}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )

            output = self.llm(
                prompt,
                max_tokens=512,
                stop=["<|im_end|>", "\n\n"],
                echo=False,
                temperature=0.1,
            )

            try:
                return output["choices"][0]["text"].strip()
            except Exception as e:
                log_message(f"Sakura output error: {e}")
                return text
